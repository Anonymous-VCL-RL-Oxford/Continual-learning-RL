{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from agents.q_learner import Q_learner\n",
    "from utils.cartpole import CartPoleEnv\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict()\n",
    "args[\"BUFFER_SIZE\"] = int(500)  # replay buffer size\n",
    "args[\"BATCH_SIZE\"] = 32  # minibatch size\n",
    "args[\"GAMMA\"] = 0.95  # discount factor\n",
    "args[\"TAU\"] = 1e-3  # for soft update of target parameters\n",
    "args[\"LR\"] = 0.001  # learning rate\n",
    "args[\"UPDATE_EVERY\"] = 4  # how often to update the network\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "\n",
    "def my_product(inp):\n",
    "    return (dict(zip(inp.keys(), values)) for values in product(*inp.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dict_to_tuple(param):\n",
    "    param_list = []\n",
    "    if \"seed\" not in param.keys():\n",
    "        param_list += [0]\n",
    "    else:\n",
    "        param_list += [param[\"seed\"]]\n",
    "        \n",
    "    if \"length\" not in param.keys():\n",
    "        param_list += [0.5]\n",
    "    else:\n",
    "        param_list += [param[\"length\"]]\n",
    "        \n",
    "    if \"gravity\" not in param.keys():\n",
    "        param_list += [9.8]\n",
    "    else:\n",
    "        param_list += [param[\"gravity\"]]\n",
    "        \n",
    "    if \"force_mag\" not in param.keys():\n",
    "        param_list += [10.0]\n",
    "    else:\n",
    "        param_list += [param[\"force_mag\"]]\n",
    "    return tuple(param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task_Wrapper():\n",
    "    def __init__(self, env_name, params):\n",
    "        self.env_name = env_name\n",
    "        self.params = list(my_product(params))\n",
    "        self.current_param = 0\n",
    "        self.seed = seed\n",
    "        self.envs = []\n",
    "        \n",
    "    def next_task(self):\n",
    "        params = self.params[self.current_param]\n",
    "        params_tuple = transform_dict_to_tuple(params)\n",
    "        env = CartPoleEnv(**params)\n",
    "        env.seed(self.seed)\n",
    "        self.current_param+=1\n",
    "        self.envs.append({params_tuple : env})\n",
    "        return self.envs\n",
    "    \n",
    "    def get_env(self, index):\n",
    "        params = self.params[index]\n",
    "        env = CartPoleEnv(**params)\n",
    "        env.seed(self.seed)\n",
    "        return env \n",
    "\n",
    "class Queue():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity-1\n",
    "        self.queue = []\n",
    "        self.nb_elems = -1\n",
    "        \n",
    "    def add(self, elem):\n",
    "        if self.nb_elems == self.capacity:\n",
    "            self.pop()\n",
    "            self.add(elem)\n",
    "        else:\n",
    "            self.queue.append(elem)\n",
    "            self.nb_elems+=1\n",
    "    \n",
    "    def pop(self):        \n",
    "        self.nb_elems -=1\n",
    "        return self.queue.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(envs, agent = None, n_episodes=10000, max_t=200, eps_start=1, eps_end=0.01, eps_decay=0.995):\n",
    "    scores_test = [Queue(50) for i in range(len(envs))]\n",
    "    scores = []                        \n",
    "    scores_window = deque(maxlen=100)  \n",
    "    eps = eps_start                    \n",
    "    env = list(envs[-1].values())[0]\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        test_dqns(scores_test, envs, agent)\n",
    "\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       \n",
    "        scores.append(score)              \n",
    "        eps = max(eps_end, eps_decay*eps)\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=195.0 and i_episode>100:\n",
    "            break\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            #torch.save(agent.qnetwork_local.state_dict(), 'models/checkpoints/checkpoint.pth')\n",
    "       \n",
    "    scores_test_list = [np.array(scores_test[i].queue).mean() for i in range(len(scores_test)) ]\n",
    "    return scores, scores_test_list\n",
    "\n",
    "\n",
    "def test_dqns(scores_test, envs, agent, n_episodes = 25, max_t = 200):\n",
    "    for i in range(len(envs)):\n",
    "        env_i = list(envs[i].values())[0]\n",
    "        scores_test[i].add(test_dqn(env_i, agent, n_episodes))\n",
    "            \n",
    "            \n",
    "def test_dqn(env, agent, n_episodes, max_t=200):\n",
    "    _scores = 0                       \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        _state = env.reset()\n",
    "        _score = 0\n",
    "        for t in range(max_t):\n",
    "            _action = agent.act(_state, 0.0)\n",
    "            _next_state, _reward, _done, _ = env.step(_action)\n",
    "            _state = _next_state\n",
    "            _score += _reward\n",
    "            if _done:\n",
    "                break \n",
    "        _scores +=  _score              \n",
    "    return _scores/n_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: (Seed, Length, Gravity, Force_mag)\n",
      "------------ Task n°1/4 ------------\n",
      "Current param: (0, 1, 9.8, 10.0)\n",
      "Episode 100\tAverage Score: 25.08\n",
      "Episode 200\tAverage Score: 20.99\n",
      "Episode 300\tAverage Score: 19.56\n",
      "Episode 400\tAverage Score: 20.40\n",
      "Episode 500\tAverage Score: 39.10\n",
      "Episode 600\tAverage Score: 87.68\n",
      "Episode 700\tAverage Score: 138.82\n",
      "Episode 800\tAverage Score: 142.89\n",
      "Episode 900\tAverage Score: 160.68\n",
      "Episode 1000\tAverage Score: 113.91\n",
      "Episode 1100\tAverage Score: 117.82\n",
      "Episode 1200\tAverage Score: 177.08\n",
      "Episode 1300\tAverage Score: 140.43\n",
      "Episode 1400\tAverage Score: 135.88\n",
      "Episode 1500\tAverage Score: 175.68\n",
      "Episode 1600\tAverage Score: 104.64\n",
      "Episode 1700\tAverage Score: 13.071\n",
      "Episode 1800\tAverage Score: 15.43\n",
      "Episode 1900\tAverage Score: 14.03\n",
      "Episode 2000\tAverage Score: 102.02\n",
      "Episode 2100\tAverage Score: 141.75\n",
      "Episode 2200\tAverage Score: 124.93\n",
      "Episode 2300\tAverage Score: 153.80\n",
      "Episode 2400\tAverage Score: 94.318\n",
      "Episode 2500\tAverage Score: 61.81\n",
      "Episode 2600\tAverage Score: 158.31\n",
      "Episode 2700\tAverage Score: 122.79\n",
      "Episode 2800\tAverage Score: 119.69\n",
      "Episode 2900\tAverage Score: 165.73\n",
      "Episode 3000\tAverage Score: 126.26\n",
      "Episode 3100\tAverage Score: 120.89\n",
      "Episode 3200\tAverage Score: 173.75\n",
      "Episode 3300\tAverage Score: 129.42\n",
      "Episode 3400\tAverage Score: 191.02\n",
      "Episode 3415\tAverage Score: 195.02[196.19760000000002]\n",
      "------------ Task n°2/4 ------------\n",
      "Current param: (0, 1, 1.62, 10.0)\n",
      "Episode 100\tAverage Score: 79.95\n",
      "Episode 200\tAverage Score: 144.25\n",
      "Episode 300\tAverage Score: 169.39\n",
      "Episode 400\tAverage Score: 181.17\n",
      "Episode 500\tAverage Score: 173.92\n",
      "Episode 600\tAverage Score: 186.29\n",
      "Episode 697\tAverage Score: 195.16[199.6416, 200.0]\n",
      "------------ Task n°3/4 ------------\n",
      "Current param: (0, 10, 9.8, 10.0)\n",
      "Episode 100\tAverage Score: 103.42\n",
      "Episode 200\tAverage Score: 105.19\n",
      "Episode 300\tAverage Score: 118.22\n",
      "Episode 400\tAverage Score: 115.95\n",
      "Episode 500\tAverage Score: 131.57\n",
      "Episode 600\tAverage Score: 167.34\n",
      "Episode 700\tAverage Score: 183.22\n",
      "Episode 800\tAverage Score: 182.16\n",
      "Episode 900\tAverage Score: 179.12\n",
      "Episode 1000\tAverage Score: 179.89\n",
      "Episode 1100\tAverage Score: 180.99\n",
      "Episode 1200\tAverage Score: 179.89\n",
      "Episode 1300\tAverage Score: 183.54\n",
      "Episode 1400\tAverage Score: 174.27\n",
      "Episode 1500\tAverage Score: 169.33\n",
      "Episode 1600\tAverage Score: 164.80\n",
      "Episode 1700\tAverage Score: 171.23\n",
      "Episode 1800\tAverage Score: 157.16\n",
      "Episode 1900\tAverage Score: 178.63\n",
      "Episode 2000\tAverage Score: 188.28\n",
      "Episode 2100\tAverage Score: 184.39\n",
      "Episode 2200\tAverage Score: 183.74\n",
      "Episode 2300\tAverage Score: 173.90\n",
      "Episode 2400\tAverage Score: 187.87\n",
      "Episode 2500\tAverage Score: 182.76\n",
      "Episode 2600\tAverage Score: 159.28\n",
      "Episode 2700\tAverage Score: 179.91\n",
      "Episode 2800\tAverage Score: 189.71\n",
      "Episode 2838\tAverage Score: 195.09[186.24719999999996, 200.0, 194.668]\n",
      "------------ Task n°4/4 ------------\n",
      "Current param: (0, 10, 1.62, 10.0)\n",
      "Episode 100\tAverage Score: 149.82\n",
      "Episode 200\tAverage Score: 188.17\n",
      "Episode 262\tAverage Score: 196.39[165.9824, 200.0, 191.2136, 200.0]\n"
     ]
    }
   ],
   "source": [
    "params = {\"length\": [1, 10], \n",
    "         \"gravity\": [9.8, 1.62],\n",
    "          \"seed\":[0]\n",
    "         }\n",
    "\n",
    "print(\"Params: (Seed, Length, Gravity, Force_mag)\")\n",
    "agent = Q_learner(state_size=4, action_size=2, seed=1, hiddens = [100,100], args = args)\n",
    "seed = 0\n",
    "\n",
    "task_wrapper = Task_Wrapper(env_name,params)\n",
    "scores = dict()\n",
    "test_scores = dict()\n",
    "for task_id in range(len(task_wrapper.params)):\n",
    "    print(\"------------ Task n°{}/{} ------------\".format(task_id+1,len(task_wrapper.params) ))\n",
    "    envs = task_wrapper.next_task()\n",
    "    param_tuple = list(envs[-1].keys())[0]\n",
    "    print(\"Current param: {}\".format(param_tuple))\n",
    "    scores[param_tuple], test_scores[param_tuple] = dqn(envs, agent)\n",
    "    print(test_scores[param_tuple])\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Save params</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "columns = [\"Task#\",\"Seed\", \"Gravity\", \"Length\", \"Force_mag\", \"Episode\", \"Score\"]\n",
    "df = pd.DataFrame(columns = columns)\n",
    "for j,param in enumerate(list(scores.keys())):\n",
    "    print(j)\n",
    "    values = scores[param]\n",
    "    liste = []\n",
    "\n",
    "    for i in range(len(values)):\n",
    "        liste.append([j, param[0], param[1], param[2],param[3], i, values[i]])\n",
    "    df2 = pd.DataFrame(data = liste, columns = columns)\n",
    "    df = pd.concat([df,df2])\n",
    "    df.reset_index()\n",
    "path= \"results/length_gravity_v2.csv\"\n",
    "df.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.  ,  1.  ,  9.8 , 10.  ],\n",
      "       [ 0.  ,  1.  ,  1.62, 10.  ],\n",
      "       [ 0.  , 10.  ,  9.8 , 10.  ],\n",
      "       [ 0.  , 10.  ,  1.62, 10.  ]])\n",
      " array([196.1976]) array([199.6416, 200.    ])\n",
      " array([186.2472, 200.    , 194.668 ])\n",
      " array([165.9824, 200.    , 191.2136, 200.    ])]\n"
     ]
    }
   ],
   "source": [
    "score_list = [np.array(list(test_scores.keys()))]\n",
    "for key in test_scores.keys():\n",
    "   \n",
    "    score_list.append(np.array(test_scores[key]))\n",
    "scores= np.array(score_list)\n",
    "print(scores)\n",
    "path= \"results/length_gravity_test_v2.npy\"\n",
    "np.save(path, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
