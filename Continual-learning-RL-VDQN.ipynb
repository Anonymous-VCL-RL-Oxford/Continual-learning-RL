{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.cartpole import CartPoleEnv\n",
    "from agents.q_learner import Q_learner\n",
    "from agents.bq_learner import BQ_learner\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict()\n",
    "args[\"BUFFER_SIZE\"] = int(500)  # replay buffer size\n",
    "args[\"BATCH_SIZE\"] = 32  # minibatch size\n",
    "args[\"GAMMA\"] = 0.95  # discount factor\n",
    "args[\"TAU\"] = 1e-3  # for soft update of target parameters\n",
    "args[\"LR\"] = 0.001  # learning rate\n",
    "args[\"UPDATE_EVERY\"] = 4  # how often to update the network\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "\n",
    "def my_product(inp):\n",
    "    return (dict(zip(inp.keys(), values)) for values in product(*inp.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dict_to_tuple(param):\n",
    "    param_list = []\n",
    "    if \"seed\" not in param.keys():\n",
    "        param_list += [0]\n",
    "    else:\n",
    "        param_list += [param[\"seed\"]]\n",
    "        \n",
    "    if \"length\" not in param.keys():\n",
    "        param_list += [0.5]\n",
    "    else:\n",
    "        param_list += [param[\"length\"]]\n",
    "        \n",
    "    if \"gravity\" not in param.keys():\n",
    "        param_list += [9.8]\n",
    "    else:\n",
    "        param_list += [param[\"gravity\"]]\n",
    "        \n",
    "    if \"force_mag\" not in param.keys():\n",
    "        param_list += [10.0]\n",
    "    else:\n",
    "        param_list += [param[\"force_mag\"]]\n",
    "    return tuple(param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task_Wrapper():\n",
    "    def __init__(self, env_name, params):\n",
    "        self.env_name = env_name\n",
    "        self.params = list(my_product(params))\n",
    "        self.current_param = 0\n",
    "        self.seed = seed\n",
    "        self.envs = []\n",
    "        \n",
    "    def next_task(self):\n",
    "        params = self.params[self.current_param]\n",
    "        params_tuple = transform_dict_to_tuple(params)\n",
    "        env = CartPoleEnv(**params)\n",
    "        env.seed(self.seed)\n",
    "        self.current_param+=1\n",
    "        self.envs.append({params_tuple : env})\n",
    "        return self.envs\n",
    "    \n",
    "    def get_env(self, index):\n",
    "        params = self.params[index]\n",
    "        env = CartPoleEnv(**params)\n",
    "        env.seed(self.seed)\n",
    "        return env \n",
    "\n",
    "class Queue():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity-1\n",
    "        self.queue = []\n",
    "        self.nb_elems = -1\n",
    "        \n",
    "    def add(self, elem):\n",
    "        if self.nb_elems == self.capacity:\n",
    "            self.pop()\n",
    "            self.add(elem)\n",
    "        else:\n",
    "            self.queue.append(elem)\n",
    "            self.nb_elems+=1\n",
    "    \n",
    "    def pop(self):        \n",
    "        self.nb_elems -=1\n",
    "        return self.queue.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(envs, agent = None, n_episodes=1000, max_t=200, eps_start=1, eps_end=0.01, eps_decay=0.995, desactivate_noise = False):\n",
    "    scores_test = [Queue(50) for i in range(len(envs))]\n",
    "    scores = []                        \n",
    "    scores_window = deque(maxlen=100)  \n",
    "    eps = eps_start                    \n",
    "    env = list(envs[-1].values())[0]\n",
    "    mean_var = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        if desactivate_noise:\n",
    "            eps = 0\n",
    "        mean_var_i  = 0\n",
    "        t_i = 0\n",
    "        for t in range(max_t):\n",
    "            t_i+=1\n",
    "            action = agent.act(state = state,task_idx = len(envs)-1, eps = eps )\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            var = agent.step(state, action, reward, next_state, done)\n",
    "            if var is not None:\n",
    "                mean_var_i += var\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        mean_var_i = mean_var_i/t_i\n",
    "        mean_var.append(mean_var_i)\n",
    "        \n",
    "        test_dqns(scores_test, envs, agent)\n",
    "\n",
    "        score_averaged = scores_test[-1].queue[-1]\n",
    "        scores_window.append(score_averaged)       \n",
    "        scores.append(score_averaged)              \n",
    "        eps = max(eps_end, eps_decay*eps)\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=195.0 and i_episode>100:\n",
    "            break\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            #torch.save(agent.qnetwork_local.state_dict(), 'models/checkpoints/checkpoint.pth')\n",
    "       \n",
    "    scores_test_list = [np.array(scores_test[i].queue).mean() for i in range(len(scores_test)) ]\n",
    "    return scores, scores_test_list, mean_var\n",
    "\n",
    "\n",
    "def test_dqns(scores_test, envs, agent, n_episodes = 3, max_t = 1000):\n",
    "    for i in range(len(envs)):\n",
    "        env_i = list(envs[i].values())[0]\n",
    "        scores_test[i].add(test_dqn(env_i, agent, i,  n_episodes, max_t))\n",
    "            \n",
    "            \n",
    "def test_dqn(env, agent, task_idx = 0, n_episodes = 1, max_t=1000):\n",
    "    _scores = 0                       \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        _state = env.reset()\n",
    "        _score = 0\n",
    "        for t in range(max_t):\n",
    "            _action = agent.act(_state, task_idx,  0.0)\n",
    "            _next_state, _reward, _done, _ = env.step(_action)\n",
    "            _state = _next_state\n",
    "            _score += _reward\n",
    "            if _done:\n",
    "                break \n",
    "        _scores +=  _score              \n",
    "    return _scores/n_episodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Agent definition</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent = Q_learner(state_size=4, action_size=2, seed=0, hiddens = [100,100], args = args)\n",
    "hiddens = [100,100]\n",
    "\n",
    "vanilla_agent = Q_learner(state_size=4, action_size=2, seed=0, hiddens = hiddens,\n",
    "                           args = args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: (Seed, Length, Gravity, Force_mag)\n",
      "------------ Task n°1/4 ------------\n",
      "Current param: (0, 1, 9.8, 10.0)\n",
      "Let's first train a Vanilla network\n",
      "Episode 100\tAverage Score: 20.11\n",
      "Episode 106\tAverage Score: 19.83"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c066ac16ada1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Let's first train a Vanilla network\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvanilla_agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-8be4edee807a>\u001b[0m in \u001b[0;36mdqn\u001b[0;34m(envs, agent, n_episodes, max_t, eps_start, eps_end, eps_decay, desactivate_noise)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mt_i\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtask_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Variational/Continual-learning-RL/agents/q_learner.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state, task_idx, eps)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = {\"length\": [1, 10], \n",
    "         \"gravity\": [9.8, 1.62],\n",
    "          \"seed\":[0]\n",
    "         }\n",
    "\n",
    "print(\"Params: (Seed, Length, Gravity, Force_mag)\")\n",
    "seed = 0\n",
    "desactivate_noise = False\n",
    "task_wrapper = Task_Wrapper(env_name,params)\n",
    "scores = dict()\n",
    "test_scores = dict()\n",
    "mean_var_tot = dict()\n",
    "for task_id in range(len(task_wrapper.params)):\n",
    "        \n",
    "    print(\"------------ Task n°{}/{} ------------\".format(task_id+1,len(task_wrapper.params) ))\n",
    "    envs = task_wrapper.next_task()\n",
    "    param_tuple = list(envs[-1].keys())[0]\n",
    "    print(\"Current param: {}\".format(param_tuple))\n",
    "\n",
    "    \n",
    "    if task_id == 0 :\n",
    "        print(\"Let's first train a Vanilla network\")\n",
    "        agent = vanilla_agent\n",
    "        _, test_score,_ = dqn(envs, agent)\n",
    "        print(test_score)\n",
    "        weights = agent.get_weights()\n",
    "        agent = BQ_learner(state_size=4, action_size=2, seed=0, hiddens = hiddens, \n",
    "                           args = args, prev_means = weights)\n",
    "        desactivate_noise = True\n",
    "        \n",
    "        print(\"We can now start using VDQNs\")\n",
    "\n",
    "    scores[param_tuple], test_scores[param_tuple], mean_var_tot[param_tuple]  = dqn(envs, agent, desactivate_noise = desactivate_noise)\n",
    "    agent.next_task()\n",
    "    print(test_scores[param_tuple])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j,param in enumerate(list(mean_var_tot.keys())):\n",
    "    values = mean_var_tot[param]\n",
    "    plt.plot([i for i in range(len(values))], values)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Task#\",\"Seed\", \"Gravity\", \"Length\", \"Force_mag\", \"Episode\", \"Score\"]\n",
    "df = pd.DataFrame(columns = columns)\n",
    "for j,param in enumerate(list(scores.keys())):\n",
    "    print(j)\n",
    "    values = scores[param]\n",
    "    liste = []\n",
    "\n",
    "    for i in range(len(values)):\n",
    "        liste.append([j, param[0], param[1], param[2],param[3], i, values[i]])\n",
    "    df2 = pd.DataFrame(data = liste, columns = columns)\n",
    "    df = pd.concat([df,df2])\n",
    "    df.reset_index()\n",
    "path= \"results/vdqn_v3.csv\"\n",
    "df.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = [np.array(list(test_scores.keys()))]\n",
    "for key in test_scores.keys():\n",
    "   \n",
    "    score_list.append(np.array(test_scores[key]))\n",
    "scores= np.array(score_list)\n",
    "print(scores)\n",
    "path= \"results/vdqn_v3.npy\"\n",
    "np.save(path, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_var_list = [np.array(list(mean_var_tot.keys()))]\n",
    "for key in mean_var_tot.keys():\n",
    "   \n",
    "    mean_var_list.append(np.array(mean_var_tot[key]))\n",
    "mean_var= np.array(mean_var_list)\n",
    "print(mean_var)\n",
    "path= \"results/vdqn_mean_var_v3.npy\"\n",
    "np.save(path, mean_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
